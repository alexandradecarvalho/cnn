{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms.functional as FF\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crowd(data.Dataset):\n",
    "    def __init__(self, root_path, crop_size, trans, downsample_ratio=8):\n",
    "        self.root_path = root_path\n",
    "        self.trans = trans\n",
    "        self.im_list = sorted(glob(os.path.join(self.root_path, '*.jpg')))\n",
    "        self.annot_list = sorted(glob(os.path.join(self.root_path, '*.mat')))\n",
    "        self.c_size = crop_size\n",
    "        print('number of img: {}'.format(len(self.im_list))) \n",
    "        \n",
    "        plt.hist(([len(loadmat(annot)['annPoints']) for annot in self.annot_list]), bins=24)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.im_list[item]\n",
    "        ann_path = img_path.replace('.jpg', '_ann.mat')\n",
    "        img = self.train_transform(Image.open(img_path).convert('RGB'))\n",
    "        keypoints = loadmat(ann_path)['annPoints']\n",
    "        img = self.trans(img)\n",
    "        \n",
    "        if len(keypoints) < 300:\n",
    "            c = 1\n",
    "        elif len(keypoints) < 700:\n",
    "            c = 2\n",
    "        elif len(keypoints) < 1200:\n",
    "            c = 3\n",
    "        elif len(keypoints) < 2300:\n",
    "            c = 4\n",
    "        else:\n",
    "            c = 5\n",
    "\n",
    "        return img, c\n",
    "\n",
    "    def train_transform(self, img):\n",
    "        img = FF.resize(img, (224,224))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of img: 1201\n",
      "number of img: 334\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 2500\n",
    "learning_rate = 0.001\n",
    "downsample_ratio=8\n",
    "\n",
    "pad_size = 9999\n",
    "\n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "train_dataset = Crowd('data/Train/', pad_size, transformation, downsample_ratio)\n",
    "test_dataset = Crowd('data/Test/', transformation, downsample_ratio)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 224, 224\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 110, 110\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 53, 53\n",
    "        x = x.view(-1, 16 * 53 * 53)            # -> n, 44944\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 12816\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1201])\n",
      "loss\n",
      "torch.Size([1201])\n",
      "loss\n",
      "torch.Size([1201])\n",
      "loss\n",
      "torch.Size([1201])\n",
      "loss\n",
      "torch.Size([1201])\n",
      "loss\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels-1)\n",
    "        print('loss')\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F45B22DFD30>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8136/353292536.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn_class_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mn_class_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8136/2292472554.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mkeypoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annPoints'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for (images, labels) in enumerate(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        #print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
